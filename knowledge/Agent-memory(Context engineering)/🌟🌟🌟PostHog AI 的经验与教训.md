## 资料
原文：[https://posthog.com/blog/8-learnings-from-1-year-of-agents-posthog-ai](https://posthog.com/blog/8-learnings-from-1-year-of-agents-posthog-ai)

## 概述
这篇技术文章概述了 PostHog AI 的发布及其一年的开发经验，该 AI 代理旨在通过多步骤分析、编写 SQL和设置实验来处理 PostHog 平台内的复杂产品数据。作者分享了核心架构见解，强调有效的代理设计依赖于持续的单一循环，而非基于图的固定工作流或分层子代理，因为单一循环能更好地保留上下文并实现自我纠正。开发的关键发现包括，使用一个简单但功能强大的 “待办事项”工具来保持大型语言模型的任务专注度，以及利用 `/init` 命令建立核心项目级内存以处理模糊的用户请求。此外，文章建议避开当前快速变化的 AI 框架，主张采用底层实现，同时指出，为了增强用户信任和理解过程，必须对用户展示所有推理步骤和工具调用。最终，PostHog 团队发现，虽然评估是必要的，但每周分析真实用户追踪记录比纯粹依赖抽象的评估指标更能准确地指导代理性能和工程决策。



## 经验一、二：单一循环胜过复杂图
![](https://cdn.nlark.com/yuque/0/2025/png/43431040/1764822765250-2d7bc0d6-e568-408e-bf97-219ed6a238dd.png)



![](https://cdn.nlark.com/yuque/0/2025/png/43431040/1764822799752-5ed9e5b7-d627-4ebc-b097-25e76b306ab6.png)



**少即是多：简单的循环远胜于复杂的架构**

我们最初认为，基于图（Graph）的工作流或专门的子代理（Subagent）是组织复杂任务的正确方式。这听起来非常“聪明”和有条理，比如设计一个“CEO 小部件（Widget CEO）”将工作分配给“工程师小部件”，再由“测试员小部件”进行验证。然而，这个想法很快暴露了其弊端。无论是复杂的图结构还是层层分包的子代理，它们都失败于同一个根本原因：上下文丢失（loss of context）。对于一个大语言模型来说，上下文就是一切。每增加一层抽象，关键信息就会被稀释，模型进行连贯推理和自我修正的能力也随之减弱，最终产出的结果，坦率地讲，是相当愚蠢的。

最终，随着模型能力的进步（例如 Anthropic 的 Claude 4 系列带来的可靠工具使用能力），我们发现最成功的架构，恰恰是那个最简单的：一个单一的循环。在这个架构中，大语言模型（LLM）不断执行任务、验证输出并自我修正。它最大程度地保留了完整的上下文，赋予了模型强大的自我纠错能力。这验证了一个经典的工程学原理：一个健壮、简单的核心，在应对未知复杂性时，往往比一个为预想场景设计的、复杂而脆弱的系统更具韧性。



## 经验三：“待办事项”是一种超能力
![](https://cdn.nlark.com/yuque/0/2025/png/43431040/1764822852168-b0a612a7-40c3-4a82-89c2-45a54c824af1.png)

在我们简单的循环架构中，有一个看似微不足道却极其强大的工具：`todo_write`。

这个工具的原理非常简单：它本身并不需要真正“做”任何事。它的唯一作用，是在每一步执行结束时，让 LLM 自己写下下一步需要做什么。这个简单的行为，却带来了惊人的效果：代理不再会在几次工具调用后迷失方向，而是可以持续不断地前进，不断修正自己的路线，完成包含数十个步骤的复杂任务。

Writing to-dos is one of those intuitive super-powers, the way chain-of-thought used to be. （“写待办事项是那种直觉性的超能力之一，就像曾经的思维链一样。”）

这个技巧的巧妙之处在于，它利用了 LLM 的内在机制。通过自我提示（self-prompting），模型能够主动维持任务的连贯性和焦点。这揭示了一个更深层的原则：在与大型模型协作时，最高效的控制方式往往不是强制性的指令，而是巧妙的引导，让模型自身的推理能力成为你最强大的盟友。

## 经验四：广泛的上下文是成功的关键
![](https://cdn.nlark.com/yuque/0/2025/png/43431040/1764822890923-a42cec6c-51c9-4bb3-8841-fc20850946c6.png)

核心上下文极大地改善了代理轨迹，但这对用户来说是一个新的任务，当新工具创建设置工作时，我们都不喜欢它。我们发现核心上下文的创建必须毫不费力，但又不强迫，这一点至关重要。最终，正如所有伟大的艺术家所做的那样，我们从最好的艺术家那里偷来的东西，并实现了从 Claude Code 抢来的 `/init` 命令。

在 PostHog AI 中，`/init` 通过多步网络搜索（当前使用 GPT-5-mini）了解您的产品和业务。结果形成了代理的项目级记忆。如果您的数据不包含可使用的 URL，系统会直接询问一些问题，但我们会尽量减少这种情况

## 经验五：展示每一步，无论是好是坏
![](https://cdn.nlark.com/yuque/0/2025/png/43431040/1764822958398-f6174e5b-88a4-4454-9359-b67632c308ca.png)

****

**建立信任的关键：展示一切，包括过程中的混乱**

为了让用户界面看起来“干净”和“专业”，我们最初隐藏了大量的“原始”细节，比如完整的推理链、失败的工具调用以及具体的参数。我们以为用户只想看到最终结果。

但用户的反馈却给了我们沉重一击：

"I can see the output, but feels hard to trust it when the process was a mystery." （“我能看到输出结果，但当整个过程是个谜团时，我很难信任它。”）

这个反馈让我们幡然醒悟。现在，PostHog AI 会实时展示每一个工具调用和推理过程的 token。我们把好的、坏的、甚至丑陋的过程全部透明地呈现给用户。这揭示了一个深刻的人机交互洞察：信任并非源于完美无瑕的结果，而是源于透明可信的过程。在一个由概率系统驱动的时代，展示“如何做”与交付“做什么”同等重要。

## 经验六：小心模型改进这台推土机
![](https://cdn.nlark.com/yuque/0/2025/png/43431040/1764823011227-55017cd9-c644-4233-93d1-8568faf3eb39.png)



一个不变的事实是：模型改进的变化比您一开始想象的要大。令人震惊的是，12 个月前推理模型仍处于试验阶段，因为如今推理对于 PostHog AI 的功能至关重要。工具的使用得到了巨大改善，因为 2025 年 11 月的前沿模型能够以更高的可靠性来理解复杂的工具。我们目前使用 Claude Sonnet 4.5 作为核心代理循环，因为它达到了质量、速度和成本的最佳点 - 但即使如此，在您意识到之前也会过时。



## 经验七：框架有害（目前而言）
![](https://cdn.nlark.com/yuque/0/2025/png/43431040/1764823084016-d296e72e-b91d-41a5-b7bb-7e22570241a8.png)

**框架的陷阱：为什么说当下的 AI 框架可能有害**

我们想直接提出一个具有争议性的观点：“Frameworks considered harmful.”（框架有害论）。

在项目早期，我们曾从 OpenAI Python SDK 迁移到 LangChain + LangGraph。但如果再选一次，我们“绝对不会”这样做。原因在于，目前的 AI 框架带来了巨大的风险：

• 锁定生态系统：使用一个框架，意味着你被锁定在它特定的思维方式和生态系统中。

• 生态系统脆弱：LLM 领域的发展速度快得惊人。框架提供的抽象层在面对底层模型的演进时往往会迅速崩溃。一个绝佳的例子是网络搜索结果的处理：OpenAI 和 Anthropic 对其格式化方式完全不同，而框架试图维持的统一门面则陷入了一片混乱。

• 重构困难：更糟糕的是，像 LangGraph 这样的编排器会让你陷入一种特定的思维模式。当这种模式被证明不再是最佳实践时，你会发现重构整个项目将是一场噩梦。



AI 领域或许有一天会迎来它的“React”，但在那一天到来之前，保持中立和底层（stay neutral and low-level）可能是最好的策略。这体现了在技术爆炸期一个永恒的权衡：过早的抽象化会扼杀适应性，而贴近底层则能保留最大的灵活性。归根结底，我们所做的大部分工作，本质上只是函数调用而已。

## 经验八： Evals 远非全部，现实世界是“粗糙”的
![](https://cdn.nlark.com/yuque/0/2025/png/43431040/1764823135548-e01fabbf-5021-4a8b-a1e9-4e5d0809b841.png)

**超越评测（Evals）：你最好的洞察力来自真实的用户轨迹**

对于基础模型的训练，评测（Evals）至关重要。但在代理（agents）开发中，它的局限性很快就显现出来。

现实世界是“粗糙”（gnarly）且充满细节的。与用户路径相对受限的传统软件不同，代理的交互本质上是用户模糊意图与模型自主解读之间的一场“谈判”。为这种多步骤的复杂任务搭建一个足够逼真的评测环境，其挑战性甚至超过了构建代理本身。Evals 往往只能覆盖你预设的“理想路径”，而真实用户总能创造出你意想不到的交互。

我们发现，比 Evals 更重要的方法是分析真实的使用情况。为此，我们团队每周都会举办“Traces Hour”会议，专门分析来自生产环境的真实用户交互的 LLM 轨迹。这些轨迹是唯一的真相来源，为我们提供了最直接、最宝贵的改进方向。这标志着 AI 产品开发的一个关键转变：重点必须从基于静态基准的验证，转向从动态、真实的用户交互中持续学习。



## 结语：不断探索的旅程
回顾这一年的历程，我们发现构建高效 AI 代理的旅程充满了反直觉的发现。其关键在于拥抱简单、追求透明，并对真实世界的复杂性抱有敬畏之心。真实世界的产品数据就像“一碗缠结的面条”，而 PostHog AI 的作用，正是解开这些缠结，重构出清晰的逻辑序列，帮助用户从混乱中找到洞察。

在构建你自己的 AI 产品时，你是否也曾陷入追求“看似聪明”的复杂性，而忽略了那些简单却更强大的原则？

虽然这些教训来之不易，但前方的挑战无疑将带来一套全新的悖论。从深度研究能力到主动的背景洞察，AI 代理的下一片前沿阵地，正等待着我们去探索和解答。

